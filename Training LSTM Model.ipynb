{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "DATASET_DIR = './data/'\n",
    "GLOVE_DIR = './glove.6B/'\n",
    "SAVE_DIR = './'\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\n",
    "y = X['domain1_score']\n",
    "X = X.dropna(axis=1)\n",
    "X = X.drop(columns=['rater1_domain1', 'rater2_domain1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>essay_id</th>\n",
       "      <th>essay_set</th>\n",
       "      <th>essay</th>\n",
       "      <th>domain1_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I think effects computer...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2, I believe that using compu...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, @CAPS1 I have found that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I know having computers has a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, I think that computers have a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>Did you know that more and more people these d...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>@PERCENT1 of people agree that computers make ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear reader, @ORGANIZATION1 has had a dramatic...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>In the @LOCATION1 we have the technology of a ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @LOCATION1, @CAPS1 people acknowledge the...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 @CAPS2 I feel that computers do ta...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper I raed ur argument on the...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>My three detaileds for this news paper article...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>15</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear, In this world today we should have every...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>16</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @ORGANIZATION1, The computer blinked to l...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, I belive that computers ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, I must admit that the ex...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>I aegre waf the evansmant ov tnachnolage. The ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>20</td>\n",
       "      <td>1</td>\n",
       "      <td>Well computers can be a good or a bad thing. I...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>21</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 of the @CAPS2 @CAPS3 daily, I am w...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local Newspaper @CAPS1 a take all your co...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>23</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, @CAPS1 you ever see a ch...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I've heard that not many...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, @CAPS2 off, I beileve that comput...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>Do you think that computers are useless? Or do...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers a good because you can get infermati...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Newspaper, Computers are high tec and hav...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>29</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, @CAPS1 people throughout...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Newspaper People, I think that computers ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>973</td>\n",
       "      <td>1</td>\n",
       "      <td>I am writing this article to inform you on how...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>974</td>\n",
       "      <td>1</td>\n",
       "      <td>What compters stop you from doing is your home...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>975</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers are a common household item these da...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>976</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear editor, I see that there is an argument b...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>977</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local newspaper, @CAPS1 opinion is yes pe...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>978</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Readers, @CAPS1 you imagine not talking t...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>979</td>\n",
       "      <td>1</td>\n",
       "      <td>To whom it @MONTH1 concern, Computers can be a...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>980</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, People spend to much tim...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>981</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear newspapper I don't think that computers b...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>982</td>\n",
       "      <td>1</td>\n",
       "      <td>In my opinion, I do believe that computers are...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>983</td>\n",
       "      <td>1</td>\n",
       "      <td>The computer is a good way to contact family a...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>984</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, I agree that computers b...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>985</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, \" Computers are great! They do so...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>986</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, @NUM1 out of @NUM2 stude...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>987</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, I am writing you this letter to g...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>988</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1, I think computers aren't good for...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>989</td>\n",
       "      <td>1</td>\n",
       "      <td>@ORGANIZATION1, In today's age computers are s...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>990</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local Newspaper, I believe the computer d...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>991</td>\n",
       "      <td>1</td>\n",
       "      <td>@ORGANIZATION1, @CAPS1 my name is @PERSON1 and...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>992</td>\n",
       "      <td>1</td>\n",
       "      <td>Computers are good to use because it tells you...</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>993</td>\n",
       "      <td>1</td>\n",
       "      <td>Have you ever wondered what life would be like...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>994</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear local newspaper, Have you even been on a ...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>995</td>\n",
       "      <td>1</td>\n",
       "      <td>I think computers are good, because some peopl...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>996</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear @CAPS1 I heard a lot of people are spendi...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>997</td>\n",
       "      <td>1</td>\n",
       "      <td>Many families across the globe use computers, ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>998</td>\n",
       "      <td>1</td>\n",
       "      <td>Imagine your life without the benifits @ORGANI...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>999</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Newspaper, @CAPS1 your life without compu...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>1000</td>\n",
       "      <td>1</td>\n",
       "      <td>The effect computers have on humans is by far ...</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>1001</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Newspaper, I think people should use comp...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>1002</td>\n",
       "      <td>1</td>\n",
       "      <td>Dear Local newspaper, I think computers are gr...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     essay_id  essay_set                                              essay  \\\n",
       "0           1          1  Dear local newspaper, I think effects computer...   \n",
       "1           2          1  Dear @CAPS1 @CAPS2, I believe that using compu...   \n",
       "2           3          1  Dear, @CAPS1 @CAPS2 @CAPS3 More and more peopl...   \n",
       "3           4          1  Dear Local Newspaper, @CAPS1 I have found that...   \n",
       "4           5          1  Dear @LOCATION1, I know having computers has a...   \n",
       "5           6          1  Dear @LOCATION1, I think that computers have a...   \n",
       "6           7          1  Did you know that more and more people these d...   \n",
       "7           8          1  @PERCENT1 of people agree that computers make ...   \n",
       "8           9          1  Dear reader, @ORGANIZATION1 has had a dramatic...   \n",
       "9          10          1  In the @LOCATION1 we have the technology of a ...   \n",
       "10         11          1  Dear @LOCATION1, @CAPS1 people acknowledge the...   \n",
       "11         12          1  Dear @CAPS1 @CAPS2 I feel that computers do ta...   \n",
       "12         13          1  Dear local newspaper I raed ur argument on the...   \n",
       "13         14          1  My three detaileds for this news paper article...   \n",
       "14         15          1  Dear, In this world today we should have every...   \n",
       "15         16          1  Dear @ORGANIZATION1, The computer blinked to l...   \n",
       "16         17          1  Dear Local Newspaper, I belive that computers ...   \n",
       "17         18          1  Dear Local Newspaper, I must admit that the ex...   \n",
       "18         19          1  I aegre waf the evansmant ov tnachnolage. The ...   \n",
       "19         20          1  Well computers can be a good or a bad thing. I...   \n",
       "20         21          1  Dear @CAPS1 of the @CAPS2 @CAPS3 daily, I am w...   \n",
       "21         22          1  Dear local Newspaper @CAPS1 a take all your co...   \n",
       "22         23          1  Dear local newspaper, @CAPS1 you ever see a ch...   \n",
       "23         24          1  Dear local newspaper, I've heard that not many...   \n",
       "24         25          1  Dear @CAPS1, @CAPS2 off, I beileve that comput...   \n",
       "25         26          1  Do you think that computers are useless? Or do...   \n",
       "26         27          1  Computers a good because you can get infermati...   \n",
       "27         28          1  Dear Newspaper, Computers are high tec and hav...   \n",
       "28         29          1  Dear local newspaper, @CAPS1 people throughout...   \n",
       "29         30          1  Dear Newspaper People, I think that computers ...   \n",
       "..        ...        ...                                                ...   \n",
       "970       973          1  I am writing this article to inform you on how...   \n",
       "971       974          1  What compters stop you from doing is your home...   \n",
       "972       975          1  Computers are a common household item these da...   \n",
       "973       976          1  Dear editor, I see that there is an argument b...   \n",
       "974       977          1  Dear Local newspaper, @CAPS1 opinion is yes pe...   \n",
       "975       978          1  Dear Readers, @CAPS1 you imagine not talking t...   \n",
       "976       979          1  To whom it @MONTH1 concern, Computers can be a...   \n",
       "977       980          1  Dear local newspaper, People spend to much tim...   \n",
       "978       981          1  Dear newspapper I don't think that computers b...   \n",
       "979       982          1  In my opinion, I do believe that computers are...   \n",
       "980       983          1  The computer is a good way to contact family a...   \n",
       "981       984          1  Dear local newspaper, I agree that computers b...   \n",
       "982       985          1  Dear @CAPS1, \" Computers are great! They do so...   \n",
       "983       986          1  Dear local newspaper, @NUM1 out of @NUM2 stude...   \n",
       "984       987          1  Dear @CAPS1, I am writing you this letter to g...   \n",
       "985       988          1  Dear @CAPS1, I think computers aren't good for...   \n",
       "986       989          1  @ORGANIZATION1, In today's age computers are s...   \n",
       "987       990          1  Dear Local Newspaper, I believe the computer d...   \n",
       "988       991          1  @ORGANIZATION1, @CAPS1 my name is @PERSON1 and...   \n",
       "989       992          1  Computers are good to use because it tells you...   \n",
       "990       993          1  Have you ever wondered what life would be like...   \n",
       "991       994          1  Dear local newspaper, Have you even been on a ...   \n",
       "992       995          1  I think computers are good, because some peopl...   \n",
       "993       996          1  Dear @CAPS1 I heard a lot of people are spendi...   \n",
       "994       997          1  Many families across the globe use computers, ...   \n",
       "995       998          1  Imagine your life without the benifits @ORGANI...   \n",
       "996       999          1  Dear Newspaper, @CAPS1 your life without compu...   \n",
       "997      1000          1  The effect computers have on humans is by far ...   \n",
       "998      1001          1  Dear Newspaper, I think people should use comp...   \n",
       "999      1002          1  Dear Local newspaper, I think computers are gr...   \n",
       "\n",
       "     domain1_score  \n",
       "0                8  \n",
       "1                9  \n",
       "2                7  \n",
       "3               10  \n",
       "4                8  \n",
       "5                8  \n",
       "6               10  \n",
       "7               10  \n",
       "8                9  \n",
       "9                9  \n",
       "10               8  \n",
       "11               8  \n",
       "12               7  \n",
       "13               6  \n",
       "14               6  \n",
       "15              12  \n",
       "16               8  \n",
       "17               8  \n",
       "18               4  \n",
       "19               6  \n",
       "20               8  \n",
       "21               3  \n",
       "22              10  \n",
       "23              11  \n",
       "24               8  \n",
       "25               9  \n",
       "26               4  \n",
       "27               9  \n",
       "28               9  \n",
       "29               8  \n",
       "..             ...  \n",
       "970              6  \n",
       "971              2  \n",
       "972              8  \n",
       "973              8  \n",
       "974              7  \n",
       "975              8  \n",
       "976             10  \n",
       "977             10  \n",
       "978              8  \n",
       "979              8  \n",
       "980              8  \n",
       "981              8  \n",
       "982              9  \n",
       "983             11  \n",
       "984              8  \n",
       "985              8  \n",
       "986              9  \n",
       "987              8  \n",
       "988              6  \n",
       "989              5  \n",
       "990              9  \n",
       "991              9  \n",
       "992              4  \n",
       "993              7  \n",
       "994             10  \n",
       "995              9  \n",
       "996             12  \n",
       "997             12  \n",
       "998              8  \n",
       "999              8  \n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.head(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I do not belive that they should be takein off the shelfs beacuse @CAPS1 people still come to get that sort of stuff. @CAPS1 people say is not plite for little kids 'well keek your kids away from it then.'             @CAPS1 people dont like it and @CAPS1 people do. I am one of the people that like that kind of stuff, beacuse no one can tell us what we can and cant watch or say. It is our life if we wanna watch that kind of stuff or read about it we can.            Noone is telling people that they half to read it or even look at it. If you dont like it dont look at it. @CAPS2 kind of stuff should be off to the side where you know what your goin to look at and away from kids                       @CAPS2 shouldnt even be a problem. @CAPS2 is just people that think they are high class and think if you look at that or watch that or read that you are a dirt bag. That is there opinon.             In conclusion  I think if you dont read @CAPS2 stuff or watch @CAPS2 stuff you are trying to be something you are not. Trying to act better than every cause your to high class to read something like that.  We all get a laugh out of @CAPS2 kind of stuff.\""
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.iloc[3255]['essay']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Minimum and Maximum Scores for each essay set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "minimum_scores = [-1, 2, 1, 0, 0, 0, 0, 0, 0]\n",
    "maximum_scores = [-1, 12, 6, 3, 3, 4, 4, 30, 60]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will preprocess all essays and convert them to feature vectors so that they can be fed into the RNN.\n",
    "\n",
    "These are all helper functions used to clean the essays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "def essay_to_wordlist(essay_v, remove_stopwords):\n",
    "    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n",
    "    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n",
    "    words = essay_v.lower().split()\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return (words)\n",
    "\n",
    "def essay_to_sentences(essay_v, remove_stopwords):\n",
    "    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n",
    "    tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    raw_sentences = tokenizer.tokenize(essay_v.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence) > 0:\n",
    "            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n",
    "    return sentences\n",
    "\n",
    "def makeFeatureVec(words, model, num_features):\n",
    "    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    num_words = 0.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            num_words += 1\n",
    "            featureVec = np.add(featureVec,model[word])        \n",
    "    featureVec = np.divide(featureVec,num_words)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(essays, model, num_features):\n",
    "    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n",
    "    counter = 0\n",
    "    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n",
    "    for essay in essays:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n",
    "        counter = counter + 1\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a 2-Layer LSTM Model. \n",
    "\n",
    "Note that instead of using sigmoid activation in the output layer we will use\n",
    "Relu since we are not normalising training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\n",
    "from keras.models import Sequential, load_model, model_from_config\n",
    "import keras.backend as K\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Define the model.\"\"\"\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n",
    "    model.add(LSTM(64, recurrent_dropout=0.4))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1, activation='relu'))\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/rutvikdixit32/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/rutvikdixit32/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model on the dataset.\n",
    "\n",
    "We will use 5-Fold Cross Validation and measure the Quadratic Weighted Kappa for each fold.\n",
    "We will then calculate Average Kappa for all the folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------Fold 1--------\n",
      "\n",
      "Training Word2Vec Model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:34: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10380/10380 [==============================] - 9s 832us/step - loss: 63.7886 - mean_absolute_error: 4.3162\n",
      "Epoch 2/50\n",
      "10380/10380 [==============================] - 5s 478us/step - loss: 39.7148 - mean_absolute_error: 3.5063\n",
      "Epoch 3/50\n",
      "10380/10380 [==============================] - 5s 489us/step - loss: 33.5014 - mean_absolute_error: 3.4221\n",
      "Epoch 4/50\n",
      "10380/10380 [==============================] - 5s 495us/step - loss: 31.1160 - mean_absolute_error: 3.3741\n",
      "Epoch 5/50\n",
      "10380/10380 [==============================] - 5s 499us/step - loss: 30.0666 - mean_absolute_error: 3.3525\n",
      "Epoch 6/50\n",
      "10380/10380 [==============================] - 5s 501us/step - loss: 28.9517 - mean_absolute_error: 3.2823\n",
      "Epoch 7/50\n",
      "10380/10380 [==============================] - 5s 503us/step - loss: 27.3991 - mean_absolute_error: 3.1231\n",
      "Epoch 8/50\n",
      "10380/10380 [==============================] - 5s 502us/step - loss: 24.8488 - mean_absolute_error: 2.9322\n",
      "Epoch 9/50\n",
      "10380/10380 [==============================] - 5s 511us/step - loss: 21.9816 - mean_absolute_error: 2.7312\n",
      "Epoch 10/50\n",
      "10380/10380 [==============================] - 5s 507us/step - loss: 19.8490 - mean_absolute_error: 2.5917\n",
      "Epoch 11/50\n",
      "10380/10380 [==============================] - 5s 506us/step - loss: 17.7680 - mean_absolute_error: 2.4552\n",
      "Epoch 12/50\n",
      "10380/10380 [==============================] - 5s 506us/step - loss: 17.1249 - mean_absolute_error: 2.3881\n",
      "Epoch 13/50\n",
      "10380/10380 [==============================] - 5s 501us/step - loss: 15.8414 - mean_absolute_error: 2.3092\n",
      "Epoch 14/50\n",
      "10380/10380 [==============================] - 5s 501us/step - loss: 15.1670 - mean_absolute_error: 2.2264\n",
      "Epoch 15/50\n",
      "10380/10380 [==============================] - 5s 498us/step - loss: 15.2408 - mean_absolute_error: 2.2260\n",
      "Epoch 16/50\n",
      "10380/10380 [==============================] - 5s 502us/step - loss: 13.2095 - mean_absolute_error: 2.0846\n",
      "Epoch 17/50\n",
      "10380/10380 [==============================] - 5s 495us/step - loss: 13.6324 - mean_absolute_error: 2.1030\n",
      "Epoch 18/50\n",
      "10380/10380 [==============================] - 5s 493us/step - loss: 12.9800 - mean_absolute_error: 2.0327\n",
      "Epoch 19/50\n",
      "10380/10380 [==============================] - 5s 489us/step - loss: 12.3706 - mean_absolute_error: 1.9942\n",
      "Epoch 20/50\n",
      "10380/10380 [==============================] - 5s 490us/step - loss: 12.2821 - mean_absolute_error: 1.9894\n",
      "Epoch 21/50\n",
      "10380/10380 [==============================] - 5s 489us/step - loss: 12.3574 - mean_absolute_error: 1.9618\n",
      "Epoch 22/50\n",
      "10380/10380 [==============================] - 5s 488us/step - loss: 11.4556 - mean_absolute_error: 1.9217\n",
      "Epoch 23/50\n",
      "10380/10380 [==============================] - 5s 488us/step - loss: 11.5470 - mean_absolute_error: 1.8940\n",
      "Epoch 24/50\n",
      "10380/10380 [==============================] - 5s 504us/step - loss: 10.8718 - mean_absolute_error: 1.8686\n",
      "Epoch 25/50\n",
      "10380/10380 [==============================] - 5s 486us/step - loss: 11.0118 - mean_absolute_error: 1.8496\n",
      "Epoch 26/50\n",
      "10380/10380 [==============================] - 5s 487us/step - loss: 10.8749 - mean_absolute_error: 1.8274\n",
      "Epoch 27/50\n",
      "10380/10380 [==============================] - 5s 489us/step - loss: 10.4942 - mean_absolute_error: 1.8066\n",
      "Epoch 28/50\n",
      "10380/10380 [==============================] - 5s 489us/step - loss: 10.2912 - mean_absolute_error: 1.7685\n",
      "Epoch 29/50\n",
      "10380/10380 [==============================] - 5s 489us/step - loss: 10.3224 - mean_absolute_error: 1.7773\n",
      "Epoch 30/50\n",
      "10380/10380 [==============================] - 5s 489us/step - loss: 10.0716 - mean_absolute_error: 1.7596\n",
      "Epoch 31/50\n",
      "10380/10380 [==============================] - 5s 487us/step - loss: 10.0634 - mean_absolute_error: 1.7578\n",
      "Epoch 32/50\n",
      "10380/10380 [==============================] - 5s 494us/step - loss: 10.2015 - mean_absolute_error: 1.7460\n",
      "Epoch 33/50\n",
      "10380/10380 [==============================] - 5s 497us/step - loss: 9.7784 - mean_absolute_error: 1.7220\n",
      "Epoch 34/50\n",
      "10380/10380 [==============================] - 5s 499us/step - loss: 9.8681 - mean_absolute_error: 1.7207\n",
      "Epoch 35/50\n",
      "10380/10380 [==============================] - 5s 492us/step - loss: 9.7564 - mean_absolute_error: 1.6935\n",
      "Epoch 36/50\n",
      "10380/10380 [==============================] - 5s 497us/step - loss: 9.6363 - mean_absolute_error: 1.7000\n",
      "Epoch 37/50\n",
      "10380/10380 [==============================] - 5s 485us/step - loss: 9.7723 - mean_absolute_error: 1.7113\n",
      "Epoch 38/50\n",
      "10380/10380 [==============================] - 5s 494us/step - loss: 9.2170 - mean_absolute_error: 1.6768\n",
      "Epoch 39/50\n",
      "10380/10380 [==============================] - 5s 490us/step - loss: 9.7954 - mean_absolute_error: 1.7009\n",
      "Epoch 40/50\n",
      "10380/10380 [==============================] - 5s 490us/step - loss: 9.4078 - mean_absolute_error: 1.6773\n",
      "Epoch 41/50\n",
      "10380/10380 [==============================] - 5s 499us/step - loss: 9.3890 - mean_absolute_error: 1.6817\n",
      "Epoch 42/50\n",
      "10380/10380 [==============================] - 5s 495us/step - loss: 9.0876 - mean_absolute_error: 1.6343\n",
      "Epoch 43/50\n",
      "10380/10380 [==============================] - 5s 498us/step - loss: 9.0592 - mean_absolute_error: 1.6465\n",
      "Epoch 44/50\n",
      "10380/10380 [==============================] - 5s 494us/step - loss: 9.1525 - mean_absolute_error: 1.6641\n",
      "Epoch 45/50\n",
      "10380/10380 [==============================] - 5s 491us/step - loss: 9.1278 - mean_absolute_error: 1.6330\n",
      "Epoch 46/50\n",
      "10380/10380 [==============================] - 5s 498us/step - loss: 9.1622 - mean_absolute_error: 1.6486\n",
      "Epoch 47/50\n",
      "10380/10380 [==============================] - 5s 491us/step - loss: 9.0887 - mean_absolute_error: 1.6399\n",
      "Epoch 48/50\n",
      "10380/10380 [==============================] - 5s 492us/step - loss: 8.6652 - mean_absolute_error: 1.6138\n",
      "Epoch 49/50\n",
      "10380/10380 [==============================] - 5s 497us/step - loss: 9.0128 - mean_absolute_error: 1.6212\n",
      "Epoch 50/50\n",
      "10380/10380 [==============================] - 5s 500us/step - loss: 9.0335 - mean_absolute_error: 1.6077\n",
      "Kappa Score: 0.9636045364785669\n",
      "\n",
      "--------Fold 2--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_3 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_4 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 8s 799us/step - loss: 62.3898 - mean_absolute_error: 4.2630\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 5s 465us/step - loss: 37.9035 - mean_absolute_error: 3.4388\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 5s 486us/step - loss: 32.8072 - mean_absolute_error: 3.3881\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 5s 474us/step - loss: 30.4890 - mean_absolute_error: 3.3446\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 5s 475us/step - loss: 29.5903 - mean_absolute_error: 3.3056\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 5s 474us/step - loss: 27.0037 - mean_absolute_error: 3.1702\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 5s 477us/step - loss: 27.1279 - mean_absolute_error: 3.0877\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 5s 476us/step - loss: 25.3130 - mean_absolute_error: 2.9309\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 5s 483us/step - loss: 21.8851 - mean_absolute_error: 2.7364\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 5s 477us/step - loss: 19.5619 - mean_absolute_error: 2.5774\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 5s 478us/step - loss: 17.9113 - mean_absolute_error: 2.4474\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 5s 478us/step - loss: 16.1665 - mean_absolute_error: 2.3456\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 14.8911 - mean_absolute_error: 2.2437\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 15.1500 - mean_absolute_error: 2.2256\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 5s 490us/step - loss: 14.1784 - mean_absolute_error: 2.1725\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 13.6962 - mean_absolute_error: 2.1290\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 12.8955 - mean_absolute_error: 2.0694\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 5s 491us/step - loss: 12.8111 - mean_absolute_error: 2.0549\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 5s 489us/step - loss: 12.8229 - mean_absolute_error: 2.0495\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 5s 489us/step - loss: 12.3051 - mean_absolute_error: 1.9881\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 5s 488us/step - loss: 11.3001 - mean_absolute_error: 1.9330\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 5s 486us/step - loss: 11.6017 - mean_absolute_error: 1.9313\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 5s 491us/step - loss: 11.3352 - mean_absolute_error: 1.9215\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 10.9202 - mean_absolute_error: 1.8844\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 5s 491us/step - loss: 10.7959 - mean_absolute_error: 1.8476\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 5s 489us/step - loss: 11.2386 - mean_absolute_error: 1.8783\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 10.3313 - mean_absolute_error: 1.8034\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 5s 486us/step - loss: 10.3349 - mean_absolute_error: 1.8041\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 9.6337 - mean_absolute_error: 1.7562\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 5s 487us/step - loss: 10.0988 - mean_absolute_error: 1.7726\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 9.5872 - mean_absolute_error: 1.7422\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 9.8047 - mean_absolute_error: 1.7476\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 9.9891 - mean_absolute_error: 1.7401\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 9.5366 - mean_absolute_error: 1.7248\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 5s 502us/step - loss: 9.2049 - mean_absolute_error: 1.7082\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 9.1285 - mean_absolute_error: 1.6897\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 5s 490us/step - loss: 9.5399 - mean_absolute_error: 1.7162\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 8.7680 - mean_absolute_error: 1.6599\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 8.8760 - mean_absolute_error: 1.6567\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 8.6642 - mean_absolute_error: 1.6464\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 9.0850 - mean_absolute_error: 1.6570\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 8.7698 - mean_absolute_error: 1.6422\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 8.6420 - mean_absolute_error: 1.6290\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 8.4713 - mean_absolute_error: 1.6120\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 8.8532 - mean_absolute_error: 1.6339\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 8.5366 - mean_absolute_error: 1.6038\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 5s 509us/step - loss: 8.1927 - mean_absolute_error: 1.5900\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 5s 518us/step - loss: 8.9272 - mean_absolute_error: 1.6120\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 8.6058 - mean_absolute_error: 1.5998\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 5s 515us/step - loss: 8.2312 - mean_absolute_error: 1.5780\n",
      "Kappa Score: 0.9641308264992351\n",
      "\n",
      "--------Fold 3--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_5 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 9s 869us/step - loss: 63.3445 - mean_absolute_error: 4.3214\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 5s 490us/step - loss: 38.5363 - mean_absolute_error: 3.4865\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 5s 490us/step - loss: 32.4240 - mean_absolute_error: 3.3988\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 5s 486us/step - loss: 30.7400 - mean_absolute_error: 3.3719\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 5s 485us/step - loss: 28.7221 - mean_absolute_error: 3.2678\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 5s 487us/step - loss: 27.0938 - mean_absolute_error: 3.1910\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 5s 488us/step - loss: 25.8521 - mean_absolute_error: 3.0591\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 5s 491us/step - loss: 24.8570 - mean_absolute_error: 2.8796\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 21.9925 - mean_absolute_error: 2.7503\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 20.4410 - mean_absolute_error: 2.6139\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 5s 490us/step - loss: 17.6292 - mean_absolute_error: 2.4478\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 17.2135 - mean_absolute_error: 2.3795\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 5s 492us/step - loss: 15.7663 - mean_absolute_error: 2.2847\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 5s 491us/step - loss: 14.9698 - mean_absolute_error: 2.2237\n",
      "Epoch 15/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 5s 493us/step - loss: 14.5601 - mean_absolute_error: 2.1887\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 5s 485us/step - loss: 13.7533 - mean_absolute_error: 2.1261\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 13.5126 - mean_absolute_error: 2.1248\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 5s 489us/step - loss: 12.8947 - mean_absolute_error: 2.0630\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 12.3072 - mean_absolute_error: 2.0116\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 11.9767 - mean_absolute_error: 1.9673\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 11.9718 - mean_absolute_error: 1.9472\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 5s 491us/step - loss: 11.6310 - mean_absolute_error: 1.9348\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 11.0860 - mean_absolute_error: 1.9020\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 5s 491us/step - loss: 11.1158 - mean_absolute_error: 1.8848\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 11.3684 - mean_absolute_error: 1.8885\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 10.8810 - mean_absolute_error: 1.8510\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 10.6424 - mean_absolute_error: 1.8161\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 10.4805 - mean_absolute_error: 1.8070\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 9.9849 - mean_absolute_error: 1.7720\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 9.9722 - mean_absolute_error: 1.7912\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 9.8382 - mean_absolute_error: 1.7601\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 9.6482 - mean_absolute_error: 1.7369\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 9.6479 - mean_absolute_error: 1.7404\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 9.5753 - mean_absolute_error: 1.7162\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 9.3732 - mean_absolute_error: 1.7157\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 5s 504us/step - loss: 9.5416 - mean_absolute_error: 1.7061\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 9.1661 - mean_absolute_error: 1.6865\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 9.1713 - mean_absolute_error: 1.6894\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 9.0238 - mean_absolute_error: 1.6797\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 8.7092 - mean_absolute_error: 1.6524\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 8.8222 - mean_absolute_error: 1.6592\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 8.7202 - mean_absolute_error: 1.6384\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 8.8379 - mean_absolute_error: 1.6547\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 8.3994 - mean_absolute_error: 1.6353\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 8.6024 - mean_absolute_error: 1.6448\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 5s 502us/step - loss: 8.5762 - mean_absolute_error: 1.6226\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 8.5449 - mean_absolute_error: 1.6116\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 8.2488 - mean_absolute_error: 1.5993\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 8.4220 - mean_absolute_error: 1.6137\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 8.4301 - mean_absolute_error: 1.6054\n",
      "Kappa Score: 0.9605135949300849\n",
      "\n",
      "--------Fold 4--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_7 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_8 (LSTM)                (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 9s 837us/step - loss: 62.9541 - mean_absolute_error: 4.3179\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 38.1671 - mean_absolute_error: 3.4346\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 5s 494us/step - loss: 33.2749 - mean_absolute_error: 3.3933\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 31.1331 - mean_absolute_error: 3.3280\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 5s 490us/step - loss: 29.2152 - mean_absolute_error: 3.2481\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 28.0363 - mean_absolute_error: 3.1479\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 5s 493us/step - loss: 26.1636 - mean_absolute_error: 2.9741\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 23.0875 - mean_absolute_error: 2.7796\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 20.7959 - mean_absolute_error: 2.6352\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 17.9501 - mean_absolute_error: 2.4761\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 17.5486 - mean_absolute_error: 2.4121\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 15.8788 - mean_absolute_error: 2.3016\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 5s 521us/step - loss: 15.1993 - mean_absolute_error: 2.2291\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 14.1733 - mean_absolute_error: 2.1851\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 14.4719 - mean_absolute_error: 2.1611\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 12.8879 - mean_absolute_error: 2.0913\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 12.7040 - mean_absolute_error: 2.0675\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 5s 504us/step - loss: 12.0067 - mean_absolute_error: 2.0055\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 11.6825 - mean_absolute_error: 1.9588\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 5s 507us/step - loss: 12.2311 - mean_absolute_error: 1.9950\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 11.7051 - mean_absolute_error: 1.9475\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 11.0669 - mean_absolute_error: 1.8996\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 11.2007 - mean_absolute_error: 1.8840\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 11.1543 - mean_absolute_error: 1.8663\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 5s 506us/step - loss: 10.4069 - mean_absolute_error: 1.8316\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 9.9007 - mean_absolute_error: 1.7866\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 5s 502us/step - loss: 9.8472 - mean_absolute_error: 1.7649\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 9.7471 - mean_absolute_error: 1.7584\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 9.7258 - mean_absolute_error: 1.7373\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 10.1243 - mean_absolute_error: 1.7815\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 5s 504us/step - loss: 9.3692 - mean_absolute_error: 1.7207\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 9.3549 - mean_absolute_error: 1.7231\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 9.5550 - mean_absolute_error: 1.7178\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 5s 502us/step - loss: 8.9621 - mean_absolute_error: 1.6772\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 8.8823 - mean_absolute_error: 1.6743\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 9.2744 - mean_absolute_error: 1.6911\n",
      "Epoch 37/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 9.4885 - mean_absolute_error: 1.7133\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 5s 504us/step - loss: 8.6838 - mean_absolute_error: 1.6612\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 8.8699 - mean_absolute_error: 1.6649\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 8.5009 - mean_absolute_error: 1.6249\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 8.7445 - mean_absolute_error: 1.6485\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 8.9612 - mean_absolute_error: 1.6512\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 5s 507us/step - loss: 9.0156 - mean_absolute_error: 1.6618\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 5s 502us/step - loss: 8.4493 - mean_absolute_error: 1.6221\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 8.1805 - mean_absolute_error: 1.5927\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 5s 510us/step - loss: 8.1225 - mean_absolute_error: 1.5989\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 8.3177 - mean_absolute_error: 1.5901\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 8.2399 - mean_absolute_error: 1.5940\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 8.1018 - mean_absolute_error: 1.5893\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 5s 506us/step - loss: 7.9020 - mean_absolute_error: 1.5751\n",
      "Kappa Score: 0.9595781156987905\n",
      "\n",
      "--------Fold 5--------\n",
      "\n",
      "Training Word2Vec Model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_9 (LSTM)                (None, 1, 300)            721200    \n",
      "_________________________________________________________________\n",
      "lstm_10 (LSTM)               (None, 64)                93440     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 1)                 65        \n",
      "=================================================================\n",
      "Total params: 814,705\n",
      "Trainable params: 814,705\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/50\n",
      "10381/10381 [==============================] - 9s 858us/step - loss: 60.7013 - mean_absolute_error: 4.2721\n",
      "Epoch 2/50\n",
      "10381/10381 [==============================] - 5s 507us/step - loss: 37.6598 - mean_absolute_error: 3.4722\n",
      "Epoch 3/50\n",
      "10381/10381 [==============================] - 5s 504us/step - loss: 31.1585 - mean_absolute_error: 3.3705\n",
      "Epoch 4/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 29.5219 - mean_absolute_error: 3.3482\n",
      "Epoch 5/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 27.6198 - mean_absolute_error: 3.2598\n",
      "Epoch 6/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 25.8306 - mean_absolute_error: 3.1769\n",
      "Epoch 7/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 25.3931 - mean_absolute_error: 3.0676\n",
      "Epoch 8/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 23.5354 - mean_absolute_error: 2.9027\n",
      "Epoch 9/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 22.5167 - mean_absolute_error: 2.8175\n",
      "Epoch 10/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 21.6005 - mean_absolute_error: 2.7238\n",
      "Epoch 11/50\n",
      "10381/10381 [==============================] - 5s 502us/step - loss: 19.8186 - mean_absolute_error: 2.6354\n",
      "Epoch 12/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 18.4378 - mean_absolute_error: 2.5290\n",
      "Epoch 13/50\n",
      "10381/10381 [==============================] - 5s 497us/step - loss: 17.5735 - mean_absolute_error: 2.4568\n",
      "Epoch 14/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 15.2470 - mean_absolute_error: 2.2787\n",
      "Epoch 15/50\n",
      "10381/10381 [==============================] - 5s 495us/step - loss: 15.0922 - mean_absolute_error: 2.2313\n",
      "Epoch 16/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 13.6113 - mean_absolute_error: 2.1141\n",
      "Epoch 17/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 12.9562 - mean_absolute_error: 2.0549\n",
      "Epoch 18/50\n",
      "10381/10381 [==============================] - 5s 496us/step - loss: 11.9387 - mean_absolute_error: 1.9657\n",
      "Epoch 19/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 11.8164 - mean_absolute_error: 1.9416\n",
      "Epoch 20/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 10.5811 - mean_absolute_error: 1.8590\n",
      "Epoch 21/50\n",
      "10381/10381 [==============================] - 5s 498us/step - loss: 11.0926 - mean_absolute_error: 1.8811\n",
      "Epoch 22/50\n",
      "10381/10381 [==============================] - 5s 504us/step - loss: 10.2108 - mean_absolute_error: 1.8061\n",
      "Epoch 23/50\n",
      "10381/10381 [==============================] - 5s 503us/step - loss: 10.1195 - mean_absolute_error: 1.7890\n",
      "Epoch 24/50\n",
      "10381/10381 [==============================] - 5s 500us/step - loss: 9.4757 - mean_absolute_error: 1.7274\n",
      "Epoch 25/50\n",
      "10381/10381 [==============================] - 5s 511us/step - loss: 9.9751 - mean_absolute_error: 1.7701\n",
      "Epoch 26/50\n",
      "10381/10381 [==============================] - 5s 522us/step - loss: 9.2047 - mean_absolute_error: 1.7011\n",
      "Epoch 27/50\n",
      "10381/10381 [==============================] - 5s 511us/step - loss: 9.3323 - mean_absolute_error: 1.6976\n",
      "Epoch 28/50\n",
      "10381/10381 [==============================] - 5s 513us/step - loss: 9.8239 - mean_absolute_error: 1.7159\n",
      "Epoch 29/50\n",
      "10381/10381 [==============================] - 5s 511us/step - loss: 8.6849 - mean_absolute_error: 1.6446\n",
      "Epoch 30/50\n",
      "10381/10381 [==============================] - 5s 508us/step - loss: 8.7430 - mean_absolute_error: 1.6406\n",
      "Epoch 31/50\n",
      "10381/10381 [==============================] - 5s 507us/step - loss: 8.9855 - mean_absolute_error: 1.6498\n",
      "Epoch 32/50\n",
      "10381/10381 [==============================] - 5s 515us/step - loss: 8.6351 - mean_absolute_error: 1.6218\n",
      "Epoch 33/50\n",
      "10381/10381 [==============================] - 5s 518us/step - loss: 8.6864 - mean_absolute_error: 1.6227\n",
      "Epoch 34/50\n",
      "10381/10381 [==============================] - 5s 515us/step - loss: 8.4039 - mean_absolute_error: 1.6000\n",
      "Epoch 35/50\n",
      "10381/10381 [==============================] - 5s 510us/step - loss: 8.7639 - mean_absolute_error: 1.6292\n",
      "Epoch 36/50\n",
      "10381/10381 [==============================] - 5s 511us/step - loss: 8.2890 - mean_absolute_error: 1.6018\n",
      "Epoch 37/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10381/10381 [==============================] - 5s 505us/step - loss: 8.4577 - mean_absolute_error: 1.5915\n",
      "Epoch 38/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 8.4044 - mean_absolute_error: 1.5859\n",
      "Epoch 39/50\n",
      "10381/10381 [==============================] - 5s 499us/step - loss: 7.8931 - mean_absolute_error: 1.5758\n",
      "Epoch 40/50\n",
      "10381/10381 [==============================] - 5s 502us/step - loss: 7.8238 - mean_absolute_error: 1.5550\n",
      "Epoch 41/50\n",
      "10381/10381 [==============================] - 5s 506us/step - loss: 7.9200 - mean_absolute_error: 1.5647\n",
      "Epoch 42/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 7.4706 - mean_absolute_error: 1.5325\n",
      "Epoch 43/50\n",
      "10381/10381 [==============================] - 5s 502us/step - loss: 7.9065 - mean_absolute_error: 1.5473\n",
      "Epoch 44/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 7.7885 - mean_absolute_error: 1.5346\n",
      "Epoch 45/50\n",
      "10381/10381 [==============================] - 5s 504us/step - loss: 7.5849 - mean_absolute_error: 1.5435\n",
      "Epoch 46/50\n",
      "10381/10381 [==============================] - 5s 512us/step - loss: 7.9844 - mean_absolute_error: 1.5463\n",
      "Epoch 47/50\n",
      "10381/10381 [==============================] - 5s 504us/step - loss: 7.5228 - mean_absolute_error: 1.5244\n",
      "Epoch 48/50\n",
      "10381/10381 [==============================] - 5s 501us/step - loss: 7.6848 - mean_absolute_error: 1.5299\n",
      "Epoch 49/50\n",
      "10381/10381 [==============================] - 5s 511us/step - loss: 7.4175 - mean_absolute_error: 1.5103\n",
      "Epoch 50/50\n",
      "10381/10381 [==============================] - 5s 505us/step - loss: 7.6801 - mean_absolute_error: 1.5232\n",
      "Kappa Score: 0.9562042517185776\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True)\n",
    "results = []\n",
    "y_pred_list = []\n",
    "\n",
    "count = 1\n",
    "for traincv, testcv in cv.split(X):\n",
    "    print(\"\\n--------Fold {}--------\\n\".format(count))\n",
    "    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n",
    "    \n",
    "    train_essays = X_train['essay']\n",
    "    test_essays = X_test['essay']\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for essay in train_essays:\n",
    "            # Obtaining all sentences from the training essays.\n",
    "            sentences += essay_to_sentences(essay, remove_stopwords = True)\n",
    "            \n",
    "    # Initializing variables for word2vec model.\n",
    "    num_features = 300 \n",
    "    min_word_count = 40\n",
    "    num_workers = 4\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "\n",
    "    print(\"Training Word2Vec Model...\")\n",
    "    model = Word2Vec(sentences, workers=num_workers, size=num_features, min_count = min_word_count, window = context, sample = downsampling)\n",
    "\n",
    "    model.init_sims(replace=True)\n",
    "    model.wv.save_word2vec_format('word2vecmodel.bin', binary=True)\n",
    "\n",
    "    clean_train_essays = []\n",
    "    \n",
    "    # Generate training and testing data word vectors.\n",
    "    for essay_v in train_essays:\n",
    "        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n",
    "    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n",
    "    \n",
    "    clean_test_essays = []\n",
    "    for essay_v in test_essays:\n",
    "        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n",
    "    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n",
    "    \n",
    "    trainDataVecs = np.array(trainDataVecs)\n",
    "    testDataVecs = np.array(testDataVecs)\n",
    "    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n",
    "    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n",
    "    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n",
    "    \n",
    "    lstm_model = get_model()\n",
    "    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n",
    "    #lstm_model.load_weights('./model_weights/final_lstm.h5')\n",
    "    y_pred = lstm_model.predict(testDataVecs)\n",
    "    \n",
    "    # Save any one of the 8 models.\n",
    "    if count == 5:\n",
    "         lstm_model.save('./model_weights/final_lstm.h5')\n",
    "    \n",
    "    # Round y_pred to the nearest integer.\n",
    "    y_pred = np.around(y_pred)\n",
    "    \n",
    "    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n",
    "    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n",
    "    print(\"Kappa Score: {}\".format(result))\n",
    "    results.append(result)\n",
    "\n",
    "    count += 1\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Avg. Kappa Score is 0.961 which is the highest we have ever seen on this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Kappa score after a 5-fold cross validation:  0.9608\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Kappa score after a 5-fold cross validation: \",np.around(np.array(results).mean(),decimals=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
